# AI Agent MCP

## ğŸ”§ Project Overview

AI Agent MCP is a modular and extensible framework built using LangChain, Chroma, OpenAI, and Streamlit. It supports Retrieval-Augmented Generation (RAG), agentic planning, memory, summarization, and fallback mechanisms to enable a robust end-to-end AI assistant.

---

## ğŸ“ Project Structure

```
ai-agent-mcp/
â”œâ”€â”€ documents/                  # PDF documents to ingest
â”œâ”€â”€ vectorstore/                # Chroma vector database (persisted)
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ rag_qa.py               # RAG pipeline logic
â”‚   â”œâ”€â”€ summarizer.py           # Summarization module
â”‚   â”œâ”€â”€ planner.py              # Planning module
â”‚   â”œâ”€â”€ memory.py               # Chat memory module
â”‚   â”œâ”€â”€ fallback.py             # GPT fallback logic
â”‚   â”œâ”€â”€ config.py               # App-wide constants
â”‚   â””â”€â”€ __init__.py             # Enables module imports
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py        # Streamlit frontend UI
â”œâ”€â”€ demo_workflow.py           # CLI test workflow
â”œâ”€â”€ main.py                     # Console entrypoint
â”œâ”€â”€ .env                        # Environment variables (OpenAI key, auth)
â”œâ”€â”€ requirements.txt            # Runtime dependencies
â”œâ”€â”€ requirements.lock.txt       # Locked dependency versions (via pip freeze)
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸš€ Features and Capabilities

### âœ… Core Features

* **RAG-based Question Answering** with `langchain_community.vectorstores.Chroma`
* **Document ingestion pipeline** supporting URLs and PDFs
* **Contextual memory** via `ConversationBufferMemory`
* **Summarization** of AI answers using `ChatOpenAI`
* **Planning module** to break down complex queries into sub-steps
* **Fallback to GPT-4** if no relevant context is found in vectorstore
* **Basic Auth in Streamlit** (via `.env` username/password)
* **Chat history panel** to view conversation flow
* **Sidebar tools**: reset chat, toggle fallback, refresh vectorstore

### ğŸ” Vector Store Ingestion

* CLI (`rag_ingest.py`) and Streamlit trigger
* Fetches:

  * PDFs from `/documents`
  * URLs listed in `rag_ingest.py`
* Splits documents into chunks with `RecursiveCharacterTextSplitter`
* Stores embeddings using OpenAI + Chroma

### ğŸ§  GPT Fallback Logic

* Automatically triggered when no relevant context is found
* Response clearly labeled: `"ğŸ’¬ ChatGPT (fallback)"`
* Prevents silent failures by returning meaningful output

### ğŸ§ª CLI Test Workflow (`demo_workflow.py`)

* Tests pipeline end-to-end
* Accepts queries
* Shows RAG hit or GPT fallback

---

## ğŸ§­ Architectural Flow

```
User â†’ Streamlit UI
     â”œâ”€â”€ Login (via .env credentials)
     â”œâ”€â”€ Query â†’ Planner â†’ RAG â†’ ChatMemory â†’ Summarizer
     â””â”€â”€ (Fallback to GPT-4 if no RAG context)

                +------------+
                |  Streamlit |
                +------------+
                      |
                      v
               +---------------+
               |  Planner.py   |
               +---------------+
                      |
                      v
               +---------------+
               |   RAGQA.py    | <--- Chroma vectorstore
               +---------------+
                      |
         +------------+------------+
         |                         |
         v                         v
   No context?                Found docs
         |                         |
         v                         v
  +---------------+         +---------------+
  | Fallback.py   |         | Summarizer.py |
  +---------------+         +---------------+
         |                         |
         +------------+------------+
                      |
                      v
               +---------------+
               | ChatMemory.py |
               +---------------+
```

---

## ğŸ“œ .env Sample

```
OPENAI_API_KEY=sk-...
APP_USERNAME=admin
APP_PASSWORD=secret
CHROMA_DB_DIR=./vectorstore
```

---

## ğŸ“‹ Setup Instructions

### âœ… Step-by-step

```bash
# 1. Clone repo
$ git clone <repo-url>
$ cd ai-agent-mcp

# 2. Create virtual environment
$ python3 -m venv venv
$ source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
$ pip install -r requirements.txt
    playwright install

# 4. Set up environment
$ cp .env.example .env
# Fill in your OpenAI key, credentials

# 5. Run ingestion (CLI)
- Add PDFs to `documents/`
- Add URLs to `documents/urls.txt`
$ python modules/rag_ingest.py

# 6. Run CLI test
$ python demo_workflow.py

# 7. Run UI
$ streamlit run ui/streamlit_app.py
```

---

## ğŸ§ª Testing Checklist

* [x] CLI ingestion runs without error
* [x] CLI RAG + fallback works
* [x] Streamlit UI launches and login succeeds
* [x] Streamlit query shows RAG answer
* [x] GPT fallback triggered when needed
* [x] Vectorstore refresh from UI works

---

## ğŸ Known Issues

* `readonly database` error may occur if vectorstore is locked (workaround: delete manually or fix file permissions)
* Streamlit UI may hang if embedding generation is slow
* Ensure `rag_ingest.py` is correctly imported in Streamlit

---

## ğŸ“„ Future Enhancements (Post-MVP)

* Add LangGraph agent for long-term workflows
* Web search integration for zero-context fallback
* Streamlit UI improvements: chat bubble view, feedback options
* Support for local LLMs via Ollama or LlamaIndex
* Dockerization

---

## ğŸ§  Summary: Application Capabilities

AI Agent MCP is a modular, hybrid AI assistant framework leveraging RAG + GPT-4 fallback to build production-grade question answering systems. Its architecture enables retrieval of enterprise content (PDFs, web pages) while still falling back on GPT for open-domain questions.

The app is designed with modularity in mind: each capability (summarization, planning, RAG, memory, fallback) is encapsulated in a standalone Python module. It uses Chroma for vector storage and OpenAI embeddings for document chunk indexing. The ingestion flow supports both PDF documents and web URLs, splitting them intelligently into chunks and storing them persistently.

The RAG logic is built using LangChain's RetrievalQA and integrates a fallback that triggers OpenAIâ€™s GPT-4 when no relevant chunks are found. This ensures seamless user experience where the assistant can answer both company-specific and general knowledge questions.

Streamlit provides a simple but powerful UI to interact with the agent. Users log in via credentials stored in a `.env` file. They can enter questions, reset conversations, enable fallback, and even rebuild the vector database from within the app. All chat history is maintained per session.

The planner module can eventually be extended to LangGraph or other agentic orchestration frameworks, making this app a perfect base for agent workflows.

