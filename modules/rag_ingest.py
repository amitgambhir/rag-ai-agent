import os
import logging
import shutil

from langchain_community.document_loaders import PlaywrightURLLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv

load_dotenv()

# â”€â”€â”€ Constants â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOCS_DIR = os.path.join(BASE_DIR, "..", "documents")
URL_FILE = os.path.join(DOCS_DIR, "urls.txt")
PERSIST_DIR = os.path.join(BASE_DIR, "..", "vectorstore")

# â”€â”€â”€ Logging â”€â”€â”€
logging.basicConfig(level=logging.INFO, format="%(message)s")
log = logging.getLogger(__name__)

# â”€â”€â”€ Loaders â”€â”€â”€

def load_urls(file_path):
    if not os.path.exists(file_path):
        return []
    with open(file_path, "r") as f:
        return [line.strip() for line in f.readlines() if line.strip()]

def load_pdfs(directory):
    if not os.path.exists(directory):
        return []
    return [
        os.path.join(directory, f)
        for f in os.listdir(directory)
        if f.endswith(".pdf")
    ]

def ingest_documents(force_reload=True):
    if force_reload and os.path.exists(PERSIST_DIR):
        shutil.rmtree(PERSIST_DIR)

    documents = []

    log.info("ğŸ“¦ Starting ingestion pipeline...")

    # Load URLs
    log.info("ğŸ”— Loading web documents...")
    urls = load_urls(URL_FILE)
    for url in urls:
        try:
            log.info(f"   â†’ Fetching: {url}")
            loader = PlaywrightURLLoader([url], remove_selectors=["header", "footer", "nav"])
            docs = loader.load()
            documents.extend(docs)
            log.info(f"     âœ” Loaded {len(docs)} documents.")
        except Exception as e:
            log.warning(f"     âŒ Error loading {url}: {e}")

    # Load PDFs
    log.info("ğŸ“„ Loading PDF documents...")
    pdfs = load_pdfs(DOCS_DIR)
    for pdf in pdfs:
        try:
            log.info(f"   â†’ Reading: {pdf}")
            loader = PyPDFLoader(pdf)
            docs = loader.load()
            documents.extend(docs)
            log.info(f"     âœ” Loaded {len(docs)} pages.")
        except Exception as e:
            log.warning(f"     âŒ Error loading PDF {pdf}: {e}")

    if not documents:
        log.warning("âš ï¸ No documents found to ingest. Skipping vectorstore creation.")
        return False

    # Chunking
    log.info("âœ‚ï¸ Splitting documents into chunks...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs_split = splitter.split_documents(documents)
    log.info(f"   â†’ Total chunks created: {len(docs_split)}")

    # Embedding
    log.info("ğŸ§  Generating embeddings and building vectorstore...")
    embeddings = OpenAIEmbeddings()
    vectordb = Chroma.from_documents(
        docs_split,
        embedding=embeddings,
        persist_directory=PERSIST_DIR,
    )
    log.info(f"ğŸ’¾ Vectorstore saved at: {PERSIST_DIR}")
    log.info("âœ… Ingestion completed successfully.")

    return True

if __name__ == "__main__":
    ingest_documents()
